---
title: "p8105_hw6_hmz2105"
author: Haley Zylberberg
output: github_document

---

```{r setup}
library (tidyverse)
library(modelr)
set.seed(123)
```

## Problem 1

This problem uses data that the Washington Post gathered on homicides in 50 large U.S. cities. 

```{r import homicide data}
homicide_df = 
  read_csv("data_prob1/homicide-data.csv") |>
    janitor::clean_names() |>
    mutate(city_state = paste(city, state, sep = ", "))|>
filter(!city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO" ))|>
  mutate(
    solved_homicides = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1),
    victim_age = as.numeric(victim_age)
    )|>
    filter(!is.na(victim_age), victim_race %in% c("Black", "White"))

```

The raw data includes year of homicide, homicide victim name, race, age and sex, city, state and lat/long of the homicide, and the status of the disposition. There are 'r nrow(homicide_df)` entries. The variables from Tulsa AL, Dallax TX,Kansas City, MO  and Phoneix AZ were removed as they do not include victim race. Also made a column with a binary variable of solved vs unsolved homicides. Will limit analysis to those for whom victim_race is white or black. Will make sure that victim_age is numeric (and remove missing values). 

Next, for the city of Baltimore, MD, fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Next obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r linear regression model for Baltimore}

bmd_homicide_df = homicide_df|>
  filter(city_state == "Baltimore, MD") 

fit_bmd = glm(solved_homicides ~ victim_age + victim_sex + victim_race, data = bmd_homicide_df, family = binomial())

fit_bmd |> 
 broom::tidy() |>
   mutate(odds_ratio = exp(estimate),
        lower_ci = exp(confint.default(fit_bmd)[, 1]),
         upper_ci = exp(confint.default(fit_bmd)[, 2])) |>
  select(term, estimate, odds_ratio, lower_ci, upper_ci) |>  
  filter(term == "victim_sexMale")  |>
  knitr::kable()
```
Interpretation: If the victim is male (compared to being female), the odds of a solved homicide decrease by approximately 0.4256 times, when all other predictors are held constant.

Next create logistic regression models for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims.

```{r linear regression model for all cities}
fit_logistic <- function(x) {
  fit = glm(solved_homicides ~ victim_age + victim_sex + victim_race, 
      data = x, family = binomial())
  
  fit |>
    broom::tidy() |>
    mutate(odds_ratio = exp(estimate),
        lower_ci = exp(confint.default(fit)[, 1]),
         upper_ci = exp(confint.default(fit)[, 2]))
}

homicide_OR_df <- homicide_df |>
  nest_by(city_state) |>
  mutate(logistic_result = map(list(data), fit_logistic)) |>
  unnest(logistic_result) |>
  filter(term == "victim_sexMale")|>
  select(estimate, odds_ratio, lower_ci, upper_ci)
```

Next, create a plot that shows the estimated ORs and CIs for each city. 

```{r plot homicide estimates}
homicide_OR_df |>
  ggplot(aes(x = reorder(city_state, -odds_ratio), y = odds_ratio)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci)) +
  theme(axis.text.x = element_text(angle = 65, hjust = 1))+
  labs(x = "City State", y = "Odds Ratio for Males vs Females")
```

This plot shows the odds of a solved homicide in males compared to females when all other variables in the model are held constant. The odds of a homicide being solved in males (compared to females) is highest in Albuquerque, NM and lowest in New York, NY. 

## Problem 2

In this problem, I will use Central Park weather data to create a simple linear regression with tmax as the response and tmin and prcp as the predictors. I use 5000 bootstrap samples to calculate r squared and log (B1*B2), and then plot the distribution of these estimates.

First pull in weather data.

```{r pull in weather data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```


Next create a bootstrap with 5000 samples. Will create a dataframe that retains the r squared and the log(B1*B2) only.

```{r create bootstrap}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

bootstrap_results =
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::glance),
    results2 = map(models, broom::tidy)
  ) |> 
  select(-strap_sample, -models) |> 
  unnest(results, results2) 

plot = bootstrap_results |> 
  janitor::clean_names()|>
  filter(term != "(Intercept)")|>
  group_by(strap_number, r_squared) |> 
summarize(log_product = log(prod(abs(estimate))))
```

Next will plot the distribution of r squared.

```{r plot r squared}
plot|>
ggplot(aes(x = r_squared)) +
    geom_density() +
  labs(title = "Distribution of R-squared",
       x = "R-squared",
       y = "Frequency") +
  theme_minimal()
```

 * The density plot of r squared values shows the distribution of the goodness of fit for the linear regression models. R squared measures the proportion of the variability in tmax that is explained by the predictors tmin and prcp. In this plot, the distribution is skewed to the left but overall appears mostly normal. As the values are relatively close to 1, the model has a high goodness of fit.


Next will plot the distribution of  log(B1*B2).

```{r plot log}
plot|>
ggplot(aes(x = log_product)) +
  geom_density() +
  labs(title = "Distribution of log(B1 * B2)",
       x = "log(B1 * B2)",
       y = "Frequency") +
  theme_minimal()
```

* This density plot provides insights into the joint effect of the predictor variables (tmin and prcp) on tmax. This distribution is very skewed to the left. It appears that there is a joint effect between tmin and prcp.

Next will make a table showing the 95% confidence levels for r squared and log (B1*B2).

```{r 95% CI}
confidence_intervals <- data.frame(
  variable = c("r_squared", "log_product"),
  lower_CI = c(quantile(pull(plot, r_squared), 0.025), quantile(pull(plot, log_product), 0.025)),
  upper_CI = c(quantile(pull(plot, r_squared), 0.975), quantile(pull(plot, log_product), 0.975))
)

confidence_intervals|> 
  knitr::kable()
```


## Problem 3

In this problem I will create a linear regression model looking at predictors of a child's birth weight using a dataset 4,342 children related variables. 

First, I will import and clean the dataset, taking into acount recoding variables and creating factors.

```{r import birthweight}
birthweight_df = 
  read_csv("data/birthweight.csv") |>
    janitor::clean_names() |>
  mutate(
 babysex = case_match(
    babysex, 2~ 'female', 1 ~ 'male'),
 frace = case_match(
    frace, 9~'Unknown', 8~'Other', 4~ 'Puerto Rican', 3~ 'Asian', 2~ 'Black', 1 ~ 'White'),
  malform = case_match(
   malform, 0~ 'absent', 1 ~ 'present'),
 mrace = case_match(
    mrace, 8~'Other', 4~ 'Puerto Rican', 3~ 'Asian', 2~ 'Black', 1 ~ 'White'),
 frace = forcats::fct_relevel(frace, c("White", "Black", "Asian", "Puerto Rican", "Other")),
  mrace = forcats::fct_relevel(mrace, c("White", "Black", "Asian", "Puerto Rican"))
  )
```

The birthweight data has `r nrow(birthweight_df)` observations and `r ncol(birthweight_df)` variables named `r names(birthweight_df)`.

Next, will create a linear regression model of birthweight. Will use the variables sex of baby (babysex), length of baby (blength), gestational age of baby (gaweeks), average cigarettes mother smoked (smoken), and mother's weight gain during pregnancy (wtgain). Will choose these items as I anticipate that they will be factors that impact a child's weight. See below for a plot of the beta coefficients (estimates) and p values.

```{r linear regression model}
fit = lm(bwt ~ babysex + blength + gaweeks + smoken + wtgain, data = birthweight_df)

fit |> 
 broom::tidy() |>
  knitr::kable()
```

Model interpretations:

* If the child is male (compared to being female), and all other predictors (gestational age, child length, average number of cigarettes of the mother, and weight gain of the mother during pregnancy) are held constant, the expected increase in the child's birth weight is 18.74 grams.

* For every 1 cm increase in child length, and with all other predictors held constant, the expected increase in the child's birth weight is 124.52 grams.

* For every additional 1 week of gestational age, and with all other predictors held constant, the expected increase in the child's birth weight is 26.26 grams.

* For every additional 1 cigarette smoked per day by the mother, and with all other predictors held constant, the expected decrease in the child's birth weight is -2.846506 grams.

* For every 1 pound  gained by the mother during pregnancy, and with all other predictors held constant, the expected increase in the child's birth weight is 4.54 grams.

Next will plot fitted values vs residuals.

```{r add model residuals and fitted values and plot}
birthweight_df|> 
  modelr::add_predictions(fit) |>
   modelr::add_residuals(fit) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals")
```

In this plot, each point represents an observation. The red dashed line represents where residuals are zero and shows that for most points, the values are evenly distributed around zero. However, there appears to be outliers around 1000, 500, and around 5000 on the x axis.

Next, fit two additional linear regression models. The model titled "fit_blength_gaweeks" uses predictors child length and gestational age. The model titled "fit_interactions" uses predictors child head size, child length and child sex and also takes into account the interaction between these predictors. See the table for the estimates from these two models.

```{r compare my linear model to 2 others}
fit_blength_gaweeks = lm(bwt ~blength + gaweeks, data = birthweight_df)

fit_blength_gaweeks |> 
 broom::tidy() |>
  knitr::kable()

fit_interactions = lm(bwt ~bhead*blength + bhead*babysex + blength*babysex + blength*bhead*babysex, data = birthweight_df)

fit_interactions |> 
 broom::tidy() |>
  knitr::kable()
```

Next compare the models in terms of their ability to predict child's birth weight using cross validation. 

```{r cross validation}
cv_df = 
  crossv_mc(birthweight_df, 100) 

cv_df =
  cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df |> 
  mutate(
    fit_mod  = map(train, \(df) lm(bwt ~ babysex + blength + gaweeks + smoken + wtgain, data = df)),
    fit2_mod  = map(train, \(df) lm(bwt ~blength + gaweeks, data = df)),
    fit3_mod  = map(train, \(df) lm(bwt ~bhead*blength + bhead*babysex + blength*babysex + blength*bhead*babysex, data = df))) |> 
  mutate(
    rmse_fit = map2_dbl(fit_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_fit2 = map2_dbl(fit2_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_fit3 = map2_dbl(fit3_mod, test, \(mod, df) rmse(model = mod, data = df)))

cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

It appears that the model that takes into account the interaction terms is the best model in terms of predictions based on RMSE. A lower RMSE indicates better model performance as it means that, on average, the model's predictions are closer to the observed values. 