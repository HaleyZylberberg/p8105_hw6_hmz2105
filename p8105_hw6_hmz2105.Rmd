---
title: "p8105_hw6_hmz2105"
author: Haley Zylberberg
output: github_document

---

```{r setup}
library (tidyverse)
library(modelr)
set.seed(123)
```

## Problem 2

In this problem, I will use Central Park weather data to create a simple linear regression with tmax as the response and tmin and prcp as the predictors. I use 5000 bootstrap samples to calculate r squared and log (B1*B2), and then plot the distribution of these estimates.

First pull in weather data.

```{r pull in weather data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```


Next create a bootstrap with 5000 samples. Will create a dataframe that retains the r squared and the log(B1*B2) only.

```{r create bootstrap}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

bootstrap_results =
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::glance),
    results2 = map(models, broom::tidy)
  ) |> 
  select(-strap_sample, -models) |> 
  unnest(results, results2) 

plot = bootstrap_results |> 
  janitor::clean_names()|>
  filter(term != "(Intercept)")|>
  group_by(strap_number, r_squared) |> 
summarize(log_product = log(prod(abs(estimate))))
```

Next will plot the distribution of r squared.

```{r plot r squared}
plot|>
ggplot(aes(x = r_squared)) +
    geom_density() +
  labs(title = "Distribution of R-squared",
       x = "R-squared",
       y = "Frequency") +
  theme_minimal()
```

 * The density plot of r squared values shows the distribution of the goodness of fit for the linear regression models. R squared measures the proportion of the variability in tmax that is explained by the predictors tmin and prcp. In this plot, the distribution is skewed to the left but overall appears mostly normal. As the values are relatively close to 1, the model has a high goodness of fit.


Next will plot the distribution of  log(B1*B2).

```{r plot log}
plot|>
ggplot(aes(x = log_product)) +
  geom_density() +
  labs(title = "Distribution of log(B1 * B2)",
       x = "log(B1 * B2)",
       y = "Frequency") +
  theme_minimal()
```

* This density plot provides insights into the joint effect of the predictor variables (tmin and prcp) on tmax. This distribution is very skewed to the left. It appears that there is a joint effect between tmin and prcp.

Next will make a table showing the 95% confidence levels for r squared and log (B1*B2).

```{r 95% CI}
confidence_intervals <- data.frame(
  variable = c("r_squared", "log_product"),
  lower_CI = c(quantile(pull(plot, r_squared), 0.025), quantile(pull(plot, log_product), 0.025)),
  upper_CI = c(quantile(pull(plot, r_squared), 0.975), quantile(pull(plot, log_product), 0.975))
)

confidence_intervals|> 
  knitr::kable()
```


## Problem 3

In this problem I will create a linear regression model looking at predictors of a child's birth weight using a dataset 4,342 children related variables. 

First, I will import and clean the dataset, taking into acount recoding variables and creating factors.

```{r import birthweight}
birthweight_df = 
  read_csv("data/birthweight.csv") |>
    janitor::clean_names() |>
  mutate(
 babysex = case_match(
    babysex, 2~ 'female', 1 ~ 'male'),
 frace = case_match(
    frace, 9~'Unknown', 8~'Other', 4~ 'Puerto Rican', 3~ 'Asian', 2~ 'Black', 1 ~ 'White'),
  malform = case_match(
   malform, 0~ 'absent', 1 ~ 'present'),
 mrace = case_match(
    mrace, 8~'Other', 4~ 'Puerto Rican', 3~ 'Asian', 2~ 'Black', 1 ~ 'White'),
 frace = forcats::fct_relevel(frace, c("White", "Black", "Asian", "Puerto Rican", "Other")),
  mrace = forcats::fct_relevel(mrace, c("White", "Black", "Asian", "Puerto Rican"))
  )
```

The birthweight data has `r nrow(birthweight_df)` observations and `r ncol(birthweight_df)` variables named `r names(birthweight_df)`.

Next, will create a linear regression model of birthweight. Will use the variables sex of baby (babysex), length of baby (blength), gestational age of baby (gaweeks), average cigarettes mother smoked (smoken), and mother's weight gain during pregnancy (wtgain). Will choose these items as I anticipate that they will be factors that impact a child's weight. See below for a plot of the beta coefficients (estimates) and p values.

```{r linear regression model}
fit = lm(bwt ~ babysex + blength + gaweeks + smoken + wtgain, data = birthweight_df)

fit |> 
 broom::tidy() |>
  knitr::kable()
```

Model interpretations:

* If the child is male (compared to being female), and all other predictors (gestational age, child length, average number of cigarettes of the mother, and weight gain of the mother during pregnancy) are held constant, the expected increase in the child's birth weight is 18.74 grams.

* For every 1 cm increase in child length, and with all other predictors held constant, the expected increase in the child's birth weight is 124.52 grams.

* For every additional 1 week of gestational age, and with all other predictors held constant, the expected increase in the child's birth weight is 26.26 grams.

* For every additional 1 cigarette smoked per day by the mother, and with all other predictors held constant, the expected decrease in the child's birth weight is -2.846506 grams.

* For every 1 pound  gained by the mother during pregnancy, and with all other predictors held constant, the expected increase in the child's birth weight is 4.54 grams.

Next will plot fitted values vs residuals.

```{r add model residuals and fitted values and plot}
birthweight_df|> 
  modelr::add_predictions(fit) |>
   modelr::add_residuals(fit) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals")
```

In this plot, each point represents an observation. The red dashed line represents where residuals are zero and shows that for most points, the values are evenly distributed around zero. However, there appears to be outliers around 1000, 500, and around 5000 on the x axis.

Next, fit two additional linear regression models. The model titled "fit_blength_gaweeks" uses predictors child length and gestational age. The model titled "fit_interactions" uses predictors child head size, child length and child sex and also takes into account the interaction between these predictors. See the table for the estimates from these two models.

```{r compare my linear model to 2 others}
fit_blength_gaweeks = lm(bwt ~blength + gaweeks, data = birthweight_df)

fit_blength_gaweeks |> 
 broom::tidy() |>
  knitr::kable()

fit_interactions = lm(bwt ~bhead*blength + bhead*babysex + blength*babysex + blength*bhead*babysex, data = birthweight_df)

fit_interactions |> 
 broom::tidy() |>
  knitr::kable()
```

Next compare the models in terms of their ability to predict child's birth weight using cross validation. 

```{r cross validation}
cv_df = 
  crossv_mc(birthweight_df, 100) 

cv_df =
  cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df |> 
  mutate(
    fit_mod  = map(train, \(df) lm(bwt ~ babysex + blength + gaweeks + smoken + wtgain, data = df)),
    fit2_mod  = map(train, \(df) lm(bwt ~blength + gaweeks, data = df)),
    fit3_mod  = map(train, \(df) lm(bwt ~bhead*blength + bhead*babysex + blength*babysex + blength*bhead*babysex, data = df))) |> 
  mutate(
    rmse_fit = map2_dbl(fit_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_fit2 = map2_dbl(fit2_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_fit3 = map2_dbl(fit3_mod, test, \(mod, df) rmse(model = mod, data = df)))

cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

It appears that the model that takes into account the interaction terms is the best model in terms of predictions based on RMSE. A lower RMSE indicates better model performance as it means that, on average, the model's predictions are closer to the observed values. 