p8105_hw6_hmz2105
================
Haley Zylberberg

``` r
library (tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.3     ✔ readr     2.1.4
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.0
    ## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(modelr)
set.seed(123)
```

## Problem 2

In this problem, I will use Central Park weather data to create a simple
linear regression with tmax as the response and tmin and prcp as the
predictors. I use 5000 bootstrap samples to calculate r squared and log
(B1\*B2), and then plot the distribution of these estimates.

First pull in weather data.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

    ## using cached file: /Users/Haley/Library/Caches/org.R-project.R/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2023-10-04 17:08:38.483749 (8.527)

    ## file min/max dates: 1869-01-01 / 2023-10-31

Next create a bootstrap with 5000 samples. Will create a dataframe that
retains the r squared and the log(B1\*B2) only.

``` r
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

bootstrap_results =
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::glance),
    results2 = map(models, broom::tidy)
  ) |> 
  select(-strap_sample, -models) |> 
  unnest(results, results2) 
```

    ## Warning: `unnest()` has a new interface. See `?unnest` for details.
    ## ℹ Try `df %>% unnest(c(results, results2))`, with `mutate()` if needed.

``` r
plot = bootstrap_results |> 
  janitor::clean_names()|>
  filter(term != "(Intercept)")|>
  group_by(strap_number, r_squared) |> 
summarize(log_product = log(prod(abs(estimate))))
```

    ## `summarise()` has grouped output by 'strap_number'. You can override using the
    ## `.groups` argument.

Next will plot the distribution of r squared.

``` r
plot|>
ggplot(aes(x = r_squared)) +
    geom_density() +
  labs(title = "Distribution of R-squared",
       x = "R-squared",
       y = "Frequency") +
  theme_minimal()
```

![](p8105_hw6_hmz2105_files/figure-gfm/plot%20r%20squared-1.png)<!-- -->

- The density plot of r squared values shows the distribution of the
  goodness of fit for the linear regression models. R squared measures
  the proportion of the variability in tmax that is explained by the
  predictors tmin and prcp. In this plot, the distribution is skewed to
  the left but overall appears mostly normal. As the values are
  relatively close to 1, the model has a high goodness of fit.

Next will plot the distribution of log(B1\*B2).

``` r
plot|>
ggplot(aes(x = log_product)) +
  geom_density() +
  labs(title = "Distribution of log(B1 * B2)",
       x = "log(B1 * B2)",
       y = "Frequency") +
  theme_minimal()
```

![](p8105_hw6_hmz2105_files/figure-gfm/plot%20log-1.png)<!-- -->

- This density plot provides insights into the joint effect of the
  predictor variables (tmin and prcp) on tmax. This distribution is very
  skewed to the left. It appears that there is a joint effect between
  tmin and prcp.

Next will make a table showing the 95% confidence levels for r squared
and log (B1\*B2).

``` r
confidence_intervals <- data.frame(
  variable = c("r_squared", "log_product"),
  lower_CI = c(quantile(pull(plot, r_squared), 0.025), quantile(pull(plot, log_product), 0.025)),
  upper_CI = c(quantile(pull(plot, r_squared), 0.975), quantile(pull(plot, log_product), 0.975))
)

confidence_intervals|> 
  knitr::kable()
```

| variable    |   lower_CI |   upper_CI |
|:------------|-----------:|-----------:|
| r_squared   |  0.8882079 |  0.9402552 |
| log_product | -8.6969696 | -4.6011529 |

## Problem 3

In this problem I will create a linear regression model looking at
predictors of a child’s birth weight using a dataset 4,342 children
related variables.

First, I will import and clean the dataset, taking into acount recoding
variables and creating factors.

``` r
birthweight_df = 
  read_csv("data/birthweight.csv") |>
    janitor::clean_names() |>
  mutate(
 babysex = case_match(
    babysex, 2~ 'female', 1 ~ 'male'),
 frace = case_match(
    frace, 9~'Unknown', 8~'Other', 4~ 'Puerto Rican', 3~ 'Asian', 2~ 'Black', 1 ~ 'White'),
  malform = case_match(
   malform, 0~ 'absent', 1 ~ 'present'),
 mrace = case_match(
    mrace, 8~'Other', 4~ 'Puerto Rican', 3~ 'Asian', 2~ 'Black', 1 ~ 'White'),
 frace = forcats::fct_relevel(frace, c("White", "Black", "Asian", "Puerto Rican", "Other")),
  mrace = forcats::fct_relevel(mrace, c("White", "Black", "Asian", "Puerto Rican"))
  )
```

    ## Rows: 4342 Columns: 20
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The birthweight data has 4342 observations and 20 variables named
babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malform,
menarche, mheight, momage, mrace, parity, pnumlbw, pnumsga, ppbmi, ppwt,
smoken, wtgain.

Next, will create a linear regression model of birthweight. Will use the
variables sex of baby (babysex), length of baby (blength), gestational
age of baby (gaweeks), average cigarettes mother smoked (smoken), and
mother’s weight gain during pregnancy (wtgain). Will choose these items
as I anticipate that they will be factors that impact a child’s weight.
See below for a plot of the beta coefficients (estimates) and p values.

``` r
fit = lm(bwt ~ babysex + blength + gaweeks + smoken + wtgain, data = birthweight_df)

fit |> 
 broom::tidy() |>
  knitr::kable()
```

| term        |     estimate |  std.error |  statistic |   p.value |
|:------------|-------------:|-----------:|-----------:|----------:|
| (Intercept) | -4214.068211 | 97.5805279 | -43.185544 | 0.0000000 |
| babysexmale |    18.736381 | 10.0758382 |   1.859536 | 0.0630189 |
| blength     |   124.515381 |  2.0097351 |  61.956115 | 0.0000000 |
| gaweeks     |    26.263820 |  1.7098324 |  15.360465 | 0.0000000 |
| smoken      |    -2.846506 |  0.6780033 |  -4.198367 | 0.0000274 |
| wtgain      |     4.549174 |  0.4670203 |   9.740848 | 0.0000000 |

Model interpretations:

- If the child is male (compared to being female), and all other
  predictors (gestational age, child length, average number of
  cigarettes of the mother, and weight gain of the mother during
  pregnancy) are held constant, the expected increase in the child’s
  birth weight is 18.74 grams.

- For every 1 cm increase in child length, and with all other predictors
  held constant, the expected increase in the child’s birth weight is
  124.52 grams.

- For every additional 1 week of gestational age, and with all other
  predictors held constant, the expected increase in the child’s birth
  weight is 26.26 grams.

- For every additional 1 cigarette smoked per day by the mother, and
  with all other predictors held constant, the expected decrease in the
  child’s birth weight is -2.846506 grams.

- For every 1 pound gained by the mother during pregnancy, and with all
  other predictors held constant, the expected increase in the child’s
  birth weight is 4.54 grams.

Next will plot fitted values vs residuals.

``` r
birthweight_df|> 
  modelr::add_predictions(fit) |>
   modelr::add_residuals(fit) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals")
```

![](p8105_hw6_hmz2105_files/figure-gfm/add%20model%20residuals%20and%20fitted%20values%20and%20plot-1.png)<!-- -->

In this plot, each point represents an observation. The red dashed line
represents where residuals are zero and shows that for most points, the
values are evenly distributed around zero. However, there appears to be
outliers around 1000, 500, and around 5000 on the x axis.

Next, fit two additional linear regression models. The model titled
“fit_blength_gaweeks” uses predictors child length and gestational age.
The model titled “fit_interactions” uses predictors child head size,
child length and child sex and also takes into account the interaction
between these predictors. See the table for the estimates from these two
models.

``` r
fit_blength_gaweeks = lm(bwt ~blength + gaweeks, data = birthweight_df)

fit_blength_gaweeks |> 
 broom::tidy() |>
  knitr::kable()
```

| term        |    estimate | std.error | statistic | p.value |
|:------------|------------:|----------:|----------:|--------:|
| (Intercept) | -4347.66707 | 97.958360 | -44.38281 |       0 |
| blength     |   128.55569 |  1.989891 |  64.60439 |       0 |
| gaweeks     |    27.04673 |  1.717930 |  15.74379 |       0 |

``` r
fit_interactions = lm(bwt ~bhead*blength + bhead*babysex + blength*babysex + blength*bhead*babysex, data = birthweight_df)

fit_interactions |> 
 broom::tidy() |>
  knitr::kable()
```

| term                      |     estimate |    std.error |  statistic |   p.value |
|:--------------------------|-------------:|-------------:|-----------:|----------:|
| (Intercept)               |  -801.948671 | 1102.3077046 | -0.7275180 | 0.4669480 |
| bhead                     |   -16.597546 |   34.0916082 | -0.4868514 | 0.6263883 |
| blength                   |   -21.645964 |   23.3720477 | -0.9261475 | 0.3544209 |
| babysexmale               | -6374.868351 | 1677.7669213 | -3.7996150 | 0.0001469 |
| bhead:blength             |     3.324444 |    0.7125586 |  4.6655020 | 0.0000032 |
| bhead:babysexmale         |   198.393181 |   51.0916850 |  3.8830816 | 0.0001047 |
| blength:babysexmale       |   123.772887 |   35.1185360 |  3.5244319 | 0.0004288 |
| bhead:blength:babysexmale |    -3.878053 |    1.0566296 | -3.6702106 | 0.0002453 |

Next compare the models in terms of their ability to predict child’s
birth weight using cross validation.

``` r
cv_df = 
  crossv_mc(birthweight_df, 100) 

cv_df =
  cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df |> 
  mutate(
    fit_mod  = map(train, \(df) lm(bwt ~ babysex + blength + gaweeks + smoken + wtgain, data = df)),
    fit2_mod  = map(train, \(df) lm(bwt ~blength + gaweeks, data = df)),
    fit3_mod  = map(train, \(df) lm(bwt ~bhead*blength + bhead*babysex + blength*babysex + blength*bhead*babysex, data = df))) |> 
  mutate(
    rmse_fit = map2_dbl(fit_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_fit2 = map2_dbl(fit2_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_fit3 = map2_dbl(fit3_mod, test, \(mod, df) rmse(model = mod, data = df)))

cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

![](p8105_hw6_hmz2105_files/figure-gfm/cross%20validation-1.png)<!-- -->

It appears that the model that takes into account the interaction terms
is the best model in terms of predictions based on RMSE. A lower RMSE
indicates better model performance as it means that, on average, the
model’s predictions are closer to the observed values.
